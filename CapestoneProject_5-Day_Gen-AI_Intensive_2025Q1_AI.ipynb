{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5514f730",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T09:43:51.498884Z",
     "iopub.status.busy": "2025-04-10T09:43:51.498511Z",
     "iopub.status.idle": "2025-04-10T09:43:53.388199Z",
     "shell.execute_reply": "2025-04-10T09:43:53.387278Z"
    },
    "papermill": {
     "duration": 1.899406,
     "end_time": "2025-04-10T09:43:53.389995",
     "exception": false,
     "start_time": "2025-04-10T09:43:51.490589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65b949",
   "metadata": {
    "papermill": {
     "duration": 0.005052,
     "end_time": "2025-04-10T09:43:53.400523",
     "exception": false,
     "start_time": "2025-04-10T09:43:53.395471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🛠 *Environment Configuration and Dependencies*\n",
    "\n",
    "*This section establishes the foundational environment for our multimodal content analysis platform. We begin by configuring the Python environment and installing essential dependencies that enable various AI capabilities:*\n",
    "\n",
    "- ***Google Generative AI**:    Core library for accessing advanced language and vision models*\n",
    "- ***LangChain**:    Framework for building language model applications*\n",
    "- ***FAISS**:    Efficient similarity search and clustering of dense vectors*\n",
    "- ***PyPDF**:    PDF processing capabilities*\n",
    "- ***Pydub**:    Audio file manipulation*\n",
    "- ***Pillow**:    Image processing*\n",
    "- ***Pytube**:    YouTube video handling*\n",
    "\n",
    "*The selection of these specific packages was driven by their proven reliability in production environments and their ability to work seamlessly together in a multimodal context.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b53414c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:43:53.411572Z",
     "iopub.status.busy": "2025-04-10T09:43:53.411109Z",
     "iopub.status.idle": "2025-04-10T09:45:37.508852Z",
     "shell.execute_reply": "2025-04-10T09:45:37.507361Z"
    },
    "papermill": {
     "duration": 104.105634,
     "end_time": "2025-04-10T09:45:37.511008",
     "exception": false,
     "start_time": "2025-04-10T09:43:53.405374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qy jupyterlab jupyterlab-lsp\n",
    "\n",
    "!pip install -qU google-generativeai\n",
    "!pip install -qU langchain\n",
    "!pip install -qU langchain-community\n",
    "!pip install -qU langchain-google-genai\n",
    "!pip install -qU faiss-cpu\n",
    "!pip install -qU python-dotenv\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU chromadb\n",
    "!pip install -qU pydub\n",
    "!pip install -qU pillow\n",
    "!pip install -qU requests\n",
    "!pip install -qU streamlit\n",
    "!pip install -qU pytube\n",
    "!pip install -qU ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0268f8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:37.530590Z",
     "iopub.status.busy": "2025-04-10T09:45:37.530234Z",
     "iopub.status.idle": "2025-04-10T09:45:40.428270Z",
     "shell.execute_reply": "2025-04-10T09:45:40.427276Z"
    },
    "papermill": {
     "duration": 2.910541,
     "end_time": "2025-04-10T09:45:40.430364",
     "exception": false,
     "start_time": "2025-04-10T09:45:37.519823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "import tempfile\n",
    "import google.generativeai as genai\n",
    "from google.genai import types\n",
    "from IPython.display import Markdown, HTML, display\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7971347c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:40.454548Z",
     "iopub.status.busy": "2025-04-10T09:45:40.453995Z",
     "iopub.status.idle": "2025-04-10T09:45:40.461362Z",
     "shell.execute_reply": "2025-04-10T09:45:40.460326Z"
    },
    "papermill": {
     "duration": 0.018715,
     "end_time": "2025-04-10T09:45:40.463447",
     "exception": false,
     "start_time": "2025-04-10T09:45:40.444732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564a98c",
   "metadata": {
    "papermill": {
     "duration": 0.007803,
     "end_time": "2025-04-10T09:45:40.479804",
     "exception": false,
     "start_time": "2025-04-10T09:45:40.472001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ⚙ *API Configuration and Model Setup*\n",
    "\n",
    "*This section handles the secure configuration of the Google Generative AI API and model initialization. We implement several key security and performance considerations:*\n",
    "\n",
    "- *Secure API key management using Kaggle's secrets*\n",
    "- *Model configuration with optimized parameters for different content types*\n",
    "- *Safety settings to ensure appropriate content filtering*\n",
    "- *Temperature and token settings optimized for our use case*\n",
    "\n",
    "*The configuration ensures both security and optimal performance across different content modalities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c39b58a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:40.497392Z",
     "iopub.status.busy": "2025-04-10T09:45:40.496979Z",
     "iopub.status.idle": "2025-04-10T09:45:40.697919Z",
     "shell.execute_reply": "2025-04-10T09:45:40.697026Z"
    },
    "papermill": {
     "duration": 0.212102,
     "end_time": "2025-04-10T09:45:40.699720",
     "exception": false,
     "start_time": "2025-04-10T09:45:40.487618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get API key\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Configure the Google Generative AI\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d1f47d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:40.717615Z",
     "iopub.status.busy": "2025-04-10T09:45:40.717324Z",
     "iopub.status.idle": "2025-04-10T09:45:40.723009Z",
     "shell.execute_reply": "2025-04-10T09:45:40.722206Z"
    },
    "papermill": {
     "duration": 0.016216,
     "end_time": "2025-04-10T09:45:40.724406",
     "exception": false,
     "start_time": "2025-04-10T09:45:40.708190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# Configure the model\n",
    "text_generation_config = {\n",
    "    \"generation_config\": {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 40,\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"candidate_count\": 1,\n",
    "    },\n",
    "    \"safety_settings\": {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "text_model = genai.GenerativeModel(model_name='gemini-2.0-flash')\n",
    "vision_model = genai.GenerativeModel(model_name='gemini-2.0-flash')\n",
    "embedding_model = 'models/embedding-001'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a212df7",
   "metadata": {
    "papermill": {
     "duration": 0.007409,
     "end_time": "2025-04-10T09:45:40.739902",
     "exception": false,
     "start_time": "2025-04-10T09:45:40.732493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📃 *Document Processing Module*\n",
    "\n",
    "*The DocumentProcessor class implements a sophisticated document analysis system with the following key features:*\n",
    "\n",
    "- ***Chunk-based Processing**:    Implements recursive text splitting for efficient processing of large documents*\n",
    "- ***RAG Implementation**:    Combines retrieval and generation for context-aware responses*\n",
    "- ***Multi-format Support**:    Handles both PDF and text documents seamlessly*\n",
    "- ***Semantic Search**:    Implements keyword-based search with relevance scoring*\n",
    "\n",
    "*This module forms the foundation for our document understanding capabilities, enabling sophisticated analysis of textual content.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78b7157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:40.757757Z",
     "iopub.status.busy": "2025-04-10T09:45:40.757396Z",
     "iopub.status.idle": "2025-04-10T09:45:41.937115Z",
     "shell.execute_reply": "2025-04-10T09:45:41.936258Z"
    },
    "papermill": {
     "duration": 1.191196,
     "end_time": "2025-04-10T09:45:41.938785",
     "exception": false,
     "start_time": "2025-04-10T09:45:40.747589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema.document import Document\n",
    "import tempfile\n",
    "import re\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Document processing module for handling PDF and text documents\"\"\"\n",
    "    \n",
    "    def __init__(self, text_model=None):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, \n",
    "            chunk_overlap=100,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        self.documents = []\n",
    "        self.text_model = text_model\n",
    "        \n",
    "    def load_pdf(self, pdf_path):\n",
    "        \"\"\"Load a PDF document and process it\"\"\"\n",
    "        try:\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            self.documents.extend(documents)\n",
    "            return f\"Loaded PDF: {pdf_path} with {len(documents)} pages\"\n",
    "        except Exception as e:\n",
    "            return f\"Error loading PDF: {str(e)}\"\n",
    "    \n",
    "    def load_text(self, text_path):\n",
    "        \"\"\"Load a text document and process it\"\"\"\n",
    "        try:\n",
    "            loader = TextLoader(text_path)\n",
    "            documents = loader.load()\n",
    "            self.documents.extend(documents)\n",
    "            return f\"Loaded text file: {text_path}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error loading text file: {str(e)}\"\n",
    "    \n",
    "    def process_text_string(self, text, metadata=None):\n",
    "        \"\"\"Process a text string directly\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {\"source\": \"direct input\"}\n",
    "        \n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        # Create Document objects\n",
    "        docs = [Document(page_content=chunk, metadata=metadata) for chunk in chunks]\n",
    "        self.documents.extend(docs)\n",
    "        return f\"Processed text input with {len(docs)} chunks\"\n",
    "    \n",
    "    def process_documents(self, documents):\n",
    "        \"\"\"Add documents to the document store\"\"\"\n",
    "        if not documents:\n",
    "            return \"No documents to process\"\n",
    "            \n",
    "        self.documents.extend(documents)\n",
    "        return f\"Added {len(documents)} documents to the store\"\n",
    "    \n",
    "    def search_documents(self, query, k=5):\n",
    "        \"\"\"Search the documents using simple keyword matching\"\"\"\n",
    "        if not self.documents:\n",
    "            return [\"No documents have been processed yet\"]\n",
    "        \n",
    "        # Simple search implementation\n",
    "        query_words = re.findall(r'\\w+', query.lower())\n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            content_lower = doc.page_content.lower()\n",
    "            # Count matching words\n",
    "            score = sum(1 for word in query_words if word in content_lower)\n",
    "            if score > 0:\n",
    "                scored_docs.append((score, doc))\n",
    "        \n",
    "        # Sort by score (descending) and take top k\n",
    "        scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "        results = [doc for _, doc in scored_docs[:k]]\n",
    "        \n",
    "        return results if results else [\"No relevant documents found\"]\n",
    "    \n",
    "    def generate_rag_response(self, query, k=5):\n",
    "        \"\"\"Generate a response using RAG\"\"\"\n",
    "        if not self.documents:\n",
    "            return \"No documents have been processed yet. Please add documents first.\"\n",
    "        \n",
    "        # Search for relevant context\n",
    "        relevant_docs = self.search_documents(query, k=k)\n",
    "        \n",
    "        if not relevant_docs or relevant_docs[0] == \"No relevant documents found\":\n",
    "            return \"No relevant information found to answer the query.\"\n",
    "        \n",
    "        # Format the context\n",
    "        if isinstance(relevant_docs[0], str):\n",
    "            context_text = \"\\n\\n\".join(relevant_docs)\n",
    "        else:\n",
    "            context_text = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Create the prompt with context\n",
    "        prompt = f\"\"\"\n",
    "        The following information is relevant to the query:\n",
    "        \n",
    "        {context_text}\n",
    "        \n",
    "        Based only on the information provided above, answer the following query. If the information needed is not \n",
    "        provided in the context, state that you don't have enough information:\n",
    "        \n",
    "        Query: {query}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate response with the text model\n",
    "        try:\n",
    "            response = self.text_model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56618543",
   "metadata": {
    "papermill": {
     "duration": 0.007705,
     "end_time": "2025-04-10T09:45:41.954750",
     "exception": false,
     "start_time": "2025-04-10T09:45:41.947045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🖼 *Image Understanding Module*\n",
    "\n",
    "*The ImageProcessor class provides comprehensive image analysis capabilities:*\n",
    "\n",
    "- ***Multi-source Loading**:    Supports both local files and remote URLs*\n",
    "- ***Object Detection**:    Identifies and classifies objects with confidence scoring*\n",
    "- ***OCR Integration**:    Extracts text from images*\n",
    "- ***Structured Analysis**:    Returns results in standardized JSON format*\n",
    "\n",
    "*The implementation focuses on practical applications while maintaining flexibility for different use cases.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c386c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:41.972765Z",
     "iopub.status.busy": "2025-04-10T09:45:41.972436Z",
     "iopub.status.idle": "2025-04-10T09:45:41.982045Z",
     "shell.execute_reply": "2025-04-10T09:45:41.981111Z"
    },
    "papermill": {
     "duration": 0.020364,
     "end_time": "2025-04-10T09:45:41.983754",
     "exception": false,
     "start_time": "2025-04-10T09:45:41.963390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    \"\"\"Image processing module for analyzing and extracting information from images\"\"\"\n",
    "    \n",
    "    def __init__(self, vision_model):\n",
    "        self.model = vision_model\n",
    "    \n",
    "    def load_image_from_path(self, image_path):\n",
    "        \"\"\"Load an image from a file path\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            return f\"Error loading image: {str(e)}\"\n",
    "    \n",
    "    def load_image_from_url(self, image_url):\n",
    "        \"\"\"Load an image from a URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(io.BytesIO(response.content))\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            return f\"Error loading image from URL: {str(e)}\"\n",
    "    \n",
    "    def analyze_image(self, image, prompt=\"Describe this image in detail\"):\n",
    "        \"\"\"Analyze the image with a specific prompt\"\"\"\n",
    "        try:\n",
    "            if isinstance(image, str):\n",
    "                if image.startswith(('http://', 'https://')):\n",
    "                    image = self.load_image_from_url(image)\n",
    "                else:\n",
    "                    image = self.load_image_from_path(image)\n",
    "            \n",
    "            response = self.model.generate_content([prompt, image])\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error analyzing image: {str(e)}\"\n",
    "    \n",
    "    def extract_text_from_image(self, image):\n",
    "        \"\"\"Extract text from an image (OCR functionality)\"\"\"\n",
    "        prompt = \"Extract and transcribe all visible text from this image. Just return the text, formatted properly.\"\n",
    "        return self.analyze_image(image, prompt)\n",
    "    \n",
    "    def identify_objects(self, image):\n",
    "        \"\"\"Identify objects in the image\"\"\"\n",
    "        prompt = \"\"\"\n",
    "        Identify all objects in this image. \n",
    "        Return the response as a JSON with the following format:\n",
    "        {\n",
    "            \"objects\": [\n",
    "                {\"name\": \"object name\", \"confidence\": \"high/medium/low\"},\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        result = self.analyze_image(image, prompt)\n",
    "        \n",
    "        # Try to extract JSON from the response\n",
    "        try:\n",
    "            # Find JSON content using regex\n",
    "            json_pattern = r'(\\{[\\s\\S]*\\})'\n",
    "            match = re.search(json_pattern, result)\n",
    "            \n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "                return json.loads(json_str)\n",
    "            else:\n",
    "                return {\"objects\": [], \"raw_response\": result}\n",
    "        except:\n",
    "            return {\"objects\": [], \"raw_response\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e989f42",
   "metadata": {
    "papermill": {
     "duration": 0.007799,
     "end_time": "2025-04-10T09:45:41.999677",
     "exception": false,
     "start_time": "2025-04-10T09:45:41.991878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🎵 *Audio Analysis Module*\n",
    "\n",
    "*The AudioProcessor class implements sophisticated audio content analysis:*\n",
    "\n",
    "- ***Function-based Architecture**:    Modular design for easy extension*\n",
    "- ***Sentiment Analysis**:    Evaluates emotional content*\n",
    "- ***Speaker Identification**:    Distinguishes between different speakers*\n",
    "- ***Contextual Understanding**:    Maintains conversation history for better analysis*\n",
    "\n",
    "*This module demonstrates the practical application of function calling in AI systems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8016b9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:42.017105Z",
     "iopub.status.busy": "2025-04-10T09:45:42.016793Z",
     "iopub.status.idle": "2025-04-10T09:45:42.061925Z",
     "shell.execute_reply": "2025-04-10T09:45:42.061121Z"
    },
    "papermill": {
     "duration": 0.056316,
     "end_time": "2025-04-10T09:45:42.063865",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.007549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from collections import deque\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "# Define schema for function calling\n",
    "def transcribe_audio(audio_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transcribes the audio file at the given path.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to the audio file to transcribe\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing transcription and metadata\n",
    "    \"\"\"\n",
    "    # Placeholder implementation - in a real scenario we would use a speech-to-text API\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant that can simulate audio transcription. \n",
    "    For this simulation, pretend you're transcribing an audio file.\n",
    "    Generate a realistic transcription text that could appear in an audio file.\n",
    "    Include any background sounds or multiple speakers if appropriate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = text_model.generate_content(system_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"transcription\": response.text,\n",
    "        \"metadata\": {\n",
    "            \"file_path\": audio_path,\n",
    "            \"status\": \"completed\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "def analyze_sentiment(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze for sentiment\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing sentiment analysis results\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the sentiment of the following text. Return the result as a JSON object with \n",
    "    'sentiment' (positive, negative, or neutral), 'confidence' (0-1), and 'explanation'.\n",
    "    \n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = text_model.generate_content(prompt)\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        json_pattern = r'(\\{[\\s\\S]*\\})'\n",
    "        match = re.search(json_pattern, response.text)\n",
    "        if match:\n",
    "            return json.loads(match.group(1))\n",
    "        else:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"explanation\": \"Failed to extract proper sentiment analysis\"\n",
    "            }\n",
    "    except:\n",
    "        return {\n",
    "            \"sentiment\": \"neutral\",\n",
    "            \"confidence\": 0.5,\n",
    "            \"explanation\": \"Failed to extract proper sentiment analysis\"\n",
    "        }\n",
    "\n",
    "def identify_speakers(transcription: str, num_speakers: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Identifies different speakers in a transcription.\n",
    "    \n",
    "    Args:\n",
    "        transcription: Text transcription to analyze\n",
    "        num_speakers: Optional hint about the number of speakers\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing speaker identification results\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Identify different speakers in the following transcription.\n",
    "    {f'There are approximately {num_speakers} speakers.' if num_speakers else ''}\n",
    "    Return the result as a JSON array where each element contains 'speaker_id' and 'text'.\n",
    "    \n",
    "    Transcription: {transcription}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = text_model.generate_content(prompt)\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        json_pattern = r'(\\[[\\s\\S]*\\])'\n",
    "        match = re.search(json_pattern, response.text)\n",
    "        if match:\n",
    "            return {\"speakers\": json.loads(match.group(1))}\n",
    "        else:\n",
    "            return {\"speakers\": [], \"raw_response\": response.text}\n",
    "    except:\n",
    "        return {\"speakers\": [], \"raw_response\": response.text}\n",
    "\n",
    "# Function calling tools\n",
    "audio_tools = [\n",
    "    {\n",
    "        \"name\": \"transcribe_audio\",\n",
    "        \"description\": \"Transcribes the audio file at the given path\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"audio_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Path to the audio file to transcribe\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"audio_path\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"analyze_sentiment\",\n",
    "        \"description\": \"Analyzes the sentiment of the given text\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Text to analyze for sentiment\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"identify_speakers\",\n",
    "        \"description\": \"Identifies different speakers in a transcription\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"transcription\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Text transcription to analyze\"\n",
    "                },\n",
    "                \"num_speakers\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Optional hint about the number of speakers\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"transcription\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4606bfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:42.081629Z",
     "iopub.status.busy": "2025-04-10T09:45:42.081329Z",
     "iopub.status.idle": "2025-04-10T09:45:42.095161Z",
     "shell.execute_reply": "2025-04-10T09:45:42.094044Z"
    },
    "papermill": {
     "duration": 0.024721,
     "end_time": "2025-04-10T09:45:42.096752",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.072031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Audio processing module for transcribing and analyzing audio content\"\"\"\n",
    "    \n",
    "    def __init__(self, text_model):\n",
    "        self.model = text_model\n",
    "        self.conversation_history = deque(maxlen=10)\n",
    "        \n",
    "    def simulate_transcription(self, audio_path):\n",
    "        \"\"\"Simulate audio transcription (since we don't have actual audio files)\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Simulate transcribing an audio file at path: {audio_path}\n",
    "        Generate a realistic transcription text that might appear in this audio file.\n",
    "        Include any background sounds or multiple speakers if appropriate.\n",
    "        Keep it brief (about 3-5 sentences).\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        return {\n",
    "            \"transcription\": response.text,\n",
    "            \"metadata\": {\n",
    "                \"file_path\": audio_path,\n",
    "                \"status\": \"completed\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze sentiment of the given text\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the sentiment of the following text. Return the result as a JSON object with \n",
    "        'sentiment' (positive, negative, or neutral), 'confidence' (0-1), and 'explanation'.\n",
    "        \n",
    "        Text: {text}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        response_text = response.text\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        try:\n",
    "            json_pattern = r'(\\{[\\s\\S]*\\})'\n",
    "            match = re.search(json_pattern, response_text)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "                return json.loads(json_str)\n",
    "            else:\n",
    "                return {\n",
    "                    \"sentiment\": \"neutral\",\n",
    "                    \"confidence\": 0.5,\n",
    "                    \"explanation\": \"Failed to extract proper sentiment analysis\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"sentiment\": \"neutral\",\n",
    "                \"confidence\": 0.5,\n",
    "                \"explanation\": f\"Error in sentiment analysis: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def identify_speakers(self, transcription, num_speakers=None):\n",
    "        \"\"\"Identify different speakers in a transcription\"\"\"\n",
    "        speaker_hint = f\"There are approximately {num_speakers} speakers.\" if num_speakers else \"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Identify different speakers in the following transcription.\n",
    "        {speaker_hint}\n",
    "        Return the result as a JSON array where each element contains 'speaker_id' and 'text'.\n",
    "        \n",
    "        Transcription: {transcription}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        response_text = response.text\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        try:\n",
    "            json_pattern = r'(\\[[\\s\\S]*\\])'\n",
    "            match = re.search(json_pattern, response_text)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "                return {\"speakers\": json.loads(json_str)}\n",
    "            else:\n",
    "                return {\"speakers\": [], \"raw_response\": response_text}\n",
    "        except Exception as e:\n",
    "            return {\"speakers\": [], \"error\": str(e), \"raw_response\": response_text}\n",
    "    \n",
    "    def process_audio(self, audio_path, query):\n",
    "        \"\"\"Process audio and respond to a query\"\"\"\n",
    "        # Add the query to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # First, simulate transcription\n",
    "        transcription_result = self.simulate_transcription(audio_path)\n",
    "        transcription = transcription_result[\"transcription\"]\n",
    "        \n",
    "        # Analyze the transcription based on the query\n",
    "        if \"sentiment\" in query.lower():\n",
    "            sentiment_result = self.analyze_sentiment(transcription)\n",
    "            analysis_result = f\"Sentiment Analysis: {json.dumps(sentiment_result, indent=2)}\"\n",
    "        elif \"speaker\" in query.lower() or \"who\" in query.lower():\n",
    "            speakers_result = self.identify_speakers(transcription)\n",
    "            analysis_result = f\"Speaker Identification: {json.dumps(speakers_result, indent=2)}\"\n",
    "        else:\n",
    "            # General analysis of the transcription\n",
    "            analysis_prompt = f\"\"\"\n",
    "            The user has provided this audio transcription:\n",
    "            \n",
    "            {transcription}\n",
    "            \n",
    "            Their query is: {query}\n",
    "            \n",
    "            Please provide a helpful analysis of the transcription in response to their query.\n",
    "            \"\"\"\n",
    "            \n",
    "            analysis_response = self.model.generate_content(analysis_prompt)\n",
    "            analysis_result = analysis_response.text\n",
    "        \n",
    "        # Combine results into a final response\n",
    "        final_prompt = f\"\"\"\n",
    "        Audio File: {audio_path}\n",
    "        \n",
    "        Transcription:\n",
    "        {transcription}\n",
    "        \n",
    "        Analysis:\n",
    "        {analysis_result}\n",
    "        \n",
    "        Please provide a concise, helpful response to the user's query: \"{query}\"\n",
    "        Focus on answering their specific question about the audio.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            final_response = self.model.generate_content(final_prompt)\n",
    "            response_text = final_response.text\n",
    "            \n",
    "            # Add the response to conversation history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "            \n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing audio query: {str(e)}\"\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": error_message})\n",
    "            return error_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f5df2",
   "metadata": {
    "papermill": {
     "duration": 0.007654,
     "end_time": "2025-04-10T09:45:42.112476",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.104822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🎞 *Video Analysis Module*\n",
    "\n",
    "*The VideoProcessor class implements a comprehensive video analysis system:*\n",
    "\n",
    "- ***Multi-modal Integration**:    Combines visual and audio analysis*\n",
    "- ***Frame Analysis**:    Processes key frames for visual understanding*\n",
    "- ***Metadata Extraction**:    Retrieves and processes video information*\n",
    "- ***Simulated Processing**:    Handles limitations in the Kaggle environment*\n",
    "\n",
    "*The implementation demonstrates practical approaches to video content analysis in constrained environments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "712a51d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:42.130580Z",
     "iopub.status.busy": "2025-04-10T09:45:42.130272Z",
     "iopub.status.idle": "2025-04-10T09:45:42.157433Z",
     "shell.execute_reply": "2025-04-10T09:45:42.156553Z"
    },
    "papermill": {
     "duration": 0.038922,
     "end_time": "2025-04-10T09:45:42.159289",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.120367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import os\n",
    "\n",
    "class VideoProcessor:\n",
    "    \"\"\"Video processing module for analyzing video content (simulated)\"\"\"\n",
    "    \n",
    "    def __init__(self, image_processor, audio_processor):\n",
    "        self.image_processor = image_processor\n",
    "        self.audio_processor = audio_processor\n",
    "    \n",
    "    def simulate_video_metadata(self, youtube_url=None, video_path=None):\n",
    "        \"\"\"Simulate retrieving video metadata\"\"\"\n",
    "        if youtube_url:\n",
    "            # Extract video ID from URL\n",
    "            video_id = youtube_url.split(\"watch?v=\")[-1] if \"watch?v=\" in youtube_url else youtube_url.split(\"/\")[-1]\n",
    "            \n",
    "            # Simulate metadata based on URL\n",
    "            return {\n",
    "                \"title\": f\"Simulated Video {video_id}\",\n",
    "                \"author\": \"Simulated Channel\",\n",
    "                \"duration\": \"10:15\",\n",
    "                \"views\": \"1,245,678\",\n",
    "                \"upload_date\": \"2023-12-15\",\n",
    "                \"description\": \"This is a simulated video description for demonstration purposes.\"\n",
    "            }\n",
    "        elif video_path:\n",
    "            # Simulate metadata based on file path\n",
    "            filename = os.path.basename(video_path)\n",
    "            return {\n",
    "                \"title\": filename,\n",
    "                \"author\": \"Local User\",\n",
    "                \"duration\": \"08:30\",\n",
    "                \"file_size\": \"245.6 MB\",\n",
    "                \"resolution\": \"1920x1080\",\n",
    "                \"format\": \"MP4\",\n",
    "                \"created_date\": \"2024-01-20\"\n",
    "            }\n",
    "        else:\n",
    "            return {\"error\": \"No video source provided\"}\n",
    "    \n",
    "    def simulate_frame_analysis(self, num_frames=5):\n",
    "        \"\"\"Simulate analyzing frames from a video\"\"\"\n",
    "        frame_analyses = []\n",
    "        \n",
    "        # Generate different simulated frame analyses for different timestamps\n",
    "        timestamps = [30, 120, 210, 300, 390]\n",
    "        \n",
    "        for i in range(min(num_frames, len(timestamps))):\n",
    "            timestamp = timestamps[i]\n",
    "            minutes = timestamp // 60\n",
    "            seconds = timestamp % 60\n",
    "            \n",
    "            # Simulate different content for different frames\n",
    "            if i == 0:\n",
    "                description = \"Introduction scene with the presenter standing in front of a blue background. The presenter is wearing a professional outfit and gesturing towards what appears to be a digital presentation screen.\"\n",
    "                objects = [\n",
    "                    {\"name\": \"person\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"presentation screen\", \"confidence\": \"medium\"},\n",
    "                    {\"name\": \"microphone\", \"confidence\": \"high\"}\n",
    "                ]\n",
    "            elif i == 1:\n",
    "                description = \"A graph showing an upward trend is displayed. The graph has multiple colored lines representing different metrics. There's a legend in the bottom right corner explaining each line.\"\n",
    "                objects = [\n",
    "                    {\"name\": \"graph\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"chart legend\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"text labels\", \"confidence\": \"medium\"}\n",
    "                ]\n",
    "            elif i == 2:\n",
    "                description = \"The presenter is now demonstrating a product. The product appears to be a small electronic device with a touchscreen. The presenter is holding it and pointing to various features.\"\n",
    "                objects = [\n",
    "                    {\"name\": \"person\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"electronic device\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"touchscreen\", \"confidence\": \"medium\"},\n",
    "                    {\"name\": \"hand gesture\", \"confidence\": \"high\"}\n",
    "                ]\n",
    "            elif i == 3:\n",
    "                description = \"A comparison table is shown with competitors' products. The table has multiple rows and columns with checkmarks and X marks indicating feature availability.\"\n",
    "                objects = [\n",
    "                    {\"name\": \"table\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"checkmark\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"text\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"product icons\", \"confidence\": \"medium\"}\n",
    "                ]\n",
    "            else:\n",
    "                description = \"Closing scene with a call-to-action slide. Contact information and social media handles are displayed prominently, along with a company logo in the bottom right.\"\n",
    "                objects = [\n",
    "                    {\"name\": \"text\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"logo\", \"confidence\": \"high\"},\n",
    "                    {\"name\": \"social media icons\", \"confidence\": \"medium\"},\n",
    "                    {\"name\": \"email address\", \"confidence\": \"high\"}\n",
    "                ]\n",
    "            \n",
    "            frame_analyses.append({\n",
    "                \"timestamp\": f\"{minutes}:{seconds:02d}\",\n",
    "                \"analysis\": description,\n",
    "                \"objects\": {\"objects\": objects}\n",
    "            })\n",
    "        \n",
    "        return frame_analyses\n",
    "    \n",
    "    def simulate_audio_transcription(self):\n",
    "        \"\"\"Simulate audio transcription from a video\"\"\"\n",
    "        return \"\"\"\n",
    "        [Upbeat music playing]\n",
    "        \n",
    "        Speaker: Welcome to our product demonstration video. Today, I'm excited to show you our latest innovation that's going to revolutionize how you interact with your smart home.\n",
    "        \n",
    "        [Music fades]\n",
    "        \n",
    "        Speaker: Our new SmartHub connects all your devices seamlessly, providing a unified control center for your entire home ecosystem. Let me show you some of the key features.\n",
    "        \n",
    "        [Brief pause]\n",
    "        \n",
    "        Speaker: As you can see from this graph, our solution offers 50% faster response times compared to leading competitors. This means your commands are executed almost instantly.\n",
    "        \n",
    "        [Sound of clicking]\n",
    "        \n",
    "        Speaker: The interface is intuitive and user-friendly. Even users with minimal technical knowledge can set up and control complex automation scenarios with just a few taps.\n",
    "        \n",
    "        [Demonstration sounds]\n",
    "        \n",
    "        Speaker: Let's look at how our product compares to others in the market. As this table shows, we offer more integration options, better security features, and longer battery life.\n",
    "        \n",
    "        [Brief pause]\n",
    "        \n",
    "        Speaker: To learn more about the SmartHub and how it can transform your home, visit our website or contact our sales team using the information on screen now.\n",
    "        \n",
    "        [Upbeat music returns]\n",
    "        \n",
    "        Speaker: Thank you for watching. Don't forget to subscribe for more product updates and demonstrations!\n",
    "        \n",
    "        [Music fades out]\n",
    "        \"\"\"\n",
    "    \n",
    "    def analyze_video(self, video_path=None, youtube_url=None):\n",
    "        \"\"\"Analyze a video (simulated)\"\"\"\n",
    "        try:\n",
    "            # Get video metadata\n",
    "            video_info = self.simulate_video_metadata(youtube_url, video_path)\n",
    "            if \"error\" in video_info:\n",
    "                return video_info\n",
    "            \n",
    "            # Simulate frame analysis\n",
    "            frame_analyses = self.simulate_frame_analysis()\n",
    "            \n",
    "            # Simulate audio transcription\n",
    "            transcription = self.simulate_audio_transcription()\n",
    "            \n",
    "            # If audio processor exists, use it to analyze the transcription\n",
    "            audio_analysis = \"\"\n",
    "            if self.audio_processor:\n",
    "                temp_audio_path = os.path.join(tempfile.mkdtemp(), \"simulated_audio.wav\")\n",
    "                audio_analysis = self.audio_processor.process_audio(\n",
    "                    temp_audio_path,\n",
    "                    \"Identify the main topics discussed in this audio and summarize the key points.\"\n",
    "                )\n",
    "            else:\n",
    "                # Provide a simulated audio analysis\n",
    "                audio_analysis = \"\"\"\n",
    "                Main topics discussed in the audio:\n",
    "                1. Product introduction - A new SmartHub for smart home control\n",
    "                2. Key features - Faster response times, intuitive interface\n",
    "                3. Competitive advantages - More integration options, better security, longer battery life\n",
    "                4. Call to action - Website visit, contact sales team, subscribe for updates\n",
    "                \n",
    "                The speaker presents a new smart home control product called SmartHub, highlighting its faster response times (50% faster than competitors), user-friendly interface, and superior features compared to market alternatives. The presentation follows a standard product demonstration format with introduction, feature showcase, competitive comparison, and call to action.\n",
    "                \"\"\"\n",
    "            \n",
    "            # Generate a comprehensive analysis based on all collected information\n",
    "            title = video_info.get(\"title\", \"Untitled Video\")\n",
    "            author = video_info.get(\"author\", \"Unknown Author\")\n",
    "            \n",
    "            analysis_prompt = f\"\"\"\n",
    "            Create a comprehensive analysis of a video with the following information:\n",
    "            \n",
    "            Title: {title}\n",
    "            Author: {author}\n",
    "            \n",
    "            Frame analyses at different timestamps:\n",
    "            {json.dumps([{\n",
    "                \"timestamp\": data[\"timestamp\"],\n",
    "                \"description\": data[\"analysis\"][:100] + \"...\" if len(data[\"analysis\"]) > 100 else data[\"analysis\"],\n",
    "                \"objects\": data[\"objects\"]\n",
    "            } for data in frame_analyses], indent=2)}\n",
    "            \n",
    "            Audio transcription and analysis:\n",
    "            {audio_analysis}\n",
    "            \n",
    "            Provide a structured analysis including:\n",
    "            1. Overall video summary\n",
    "            2. Main visual elements and how they change over time\n",
    "            3. Main topics discussed in the audio\n",
    "            4. Overall mood/tone of the video\n",
    "            \"\"\"\n",
    "            \n",
    "            # Generate the final analysis using the text model\n",
    "            final_analysis_response = genai.GenerativeModel(model_name='gemini-2.0-flash').generate_content(analysis_prompt)\n",
    "            final_analysis = final_analysis_response.text\n",
    "            \n",
    "            return {\n",
    "                \"video_info\": video_info,\n",
    "                \"frame_analyses\": frame_analyses,\n",
    "                \"audio_analysis\": audio_analysis,\n",
    "                \"final_analysis\": final_analysis\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error in simulated video analysis: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0422d30",
   "metadata": {
    "papermill": {
     "duration": 0.007699,
     "end_time": "2025-04-10T09:45:42.175154",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.167455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🗜 *Multimodal Content Hub*\n",
    "\n",
    "*The MultimodalContentHub class serves as the central integration point for all content analysis capabilities:*\n",
    "\n",
    "- ***Unified Interface**:    Provides consistent access to all analysis features*\n",
    "- ***Content Integration**:    Seamlessly combines different content types*\n",
    "- ***Contextual Analysis**:    Maintains relationships between different content modalities*\n",
    "- ***Error Handling**:    Implements robust error management across all operations*\n",
    "\n",
    "*This class demonstrates the practical implementation of multimodal AI systems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae5ca94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:42.192276Z",
     "iopub.status.busy": "2025-04-10T09:45:42.191948Z",
     "iopub.status.idle": "2025-04-10T09:45:42.206232Z",
     "shell.execute_reply": "2025-04-10T09:45:42.205025Z"
    },
    "papermill": {
     "duration": 0.025223,
     "end_time": "2025-04-10T09:45:42.208107",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.182884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultimodalContentHub:\n",
    "    \"\"\"Main application class that integrates all modules\"\"\"\n",
    "    \n",
    "    def __init__(self, google_api_key):\n",
    "        # Configure Google AI\n",
    "        genai.configure(api_key=google_api_key)\n",
    "    \n",
    "        # Initialize models\n",
    "        self.text_model = genai.GenerativeModel(model_name='gemini-2.0-flash')\n",
    "        self.vision_model = genai.GenerativeModel(model_name='gemini-2.0-flash')\n",
    "    \n",
    "        # Initialize modules\n",
    "        self.document_processor = DocumentProcessor(text_model=self.text_model)\n",
    "        self.image_processor = ImageProcessor(self.vision_model)\n",
    "        self.audio_processor = AudioProcessor(self.text_model)\n",
    "        self.video_processor = VideoProcessor(self.image_processor, self.audio_processor)\n",
    "    \n",
    "    def process_document(self, document_path):\n",
    "        \"\"\"Process a document (PDF or text)\"\"\"\n",
    "        if document_path.lower().endswith('.pdf'):\n",
    "            return self.document_processor.load_pdf(document_path)\n",
    "        else:\n",
    "            return self.document_processor.load_text(document_path)\n",
    "    \n",
    "    def query_document(self, query):\n",
    "        \"\"\"Query the document knowledge base\"\"\"\n",
    "        return self.document_processor.generate_rag_response(query)\n",
    "    \n",
    "    def analyze_image(self, image_path):\n",
    "        \"\"\"Analyze an image\"\"\"\n",
    "        return self.image_processor.analyze_image(image_path)\n",
    "    \n",
    "    def extract_text_from_image(self, image_path):\n",
    "        \"\"\"Extract text from an image\"\"\"\n",
    "        return self.image_processor.extract_text_from_image(image_path)\n",
    "    \n",
    "    def analyze_audio(self, audio_path, query):\n",
    "        \"\"\"Analyze audio content\"\"\"\n",
    "        return self.audio_processor.process_audio(audio_path, query)\n",
    "    \n",
    "    def analyze_video(self, video_path=None, youtube_url=None):\n",
    "        \"\"\"Analyze video content\"\"\"\n",
    "        return self.video_processor.analyze_video(video_path, youtube_url)\n",
    "    \n",
    "    def analyze_mixed_content(self, text=None, images=None, audio=None, video=None, query=None):\n",
    "        \"\"\"Analyze mixed content types and provide a unified analysis\"\"\"\n",
    "        results = {}\n",
    "        context = []\n",
    "        \n",
    "        # Process text/documents if provided\n",
    "        if text:\n",
    "            if isinstance(text, list):\n",
    "                for text_item in text:\n",
    "                    self.document_processor.process_text_string(text_item)\n",
    "            else:\n",
    "                self.document_processor.process_text_string(text)\n",
    "                \n",
    "            if query:\n",
    "                results[\"text_analysis\"] = self.document_processor.generate_rag_response(query)\n",
    "                context.append(f\"Text Analysis: {results['text_analysis']}\")\n",
    "        \n",
    "        # Process images if provided\n",
    "        if images:\n",
    "            if not isinstance(images, list):\n",
    "                images = [images]\n",
    "                \n",
    "            image_analyses = []\n",
    "            for img in images:\n",
    "                analysis = self.image_processor.analyze_image(img)\n",
    "                image_analyses.append(analysis)\n",
    "                \n",
    "            results[\"image_analyses\"] = image_analyses\n",
    "            context.append(f\"Image Analysis: {' '.join(image_analyses)}\")\n",
    "        \n",
    "        # Process audio if provided\n",
    "        if audio:\n",
    "            if not isinstance(audio, list):\n",
    "                audio = [audio]\n",
    "                \n",
    "            audio_analyses = []\n",
    "            for audio_file in audio:\n",
    "                analysis = self.audio_processor.process_audio(\n",
    "                    audio_file, \n",
    "                    query if query else \"Transcribe and analyze this audio.\"\n",
    "                )\n",
    "                audio_analyses.append(analysis)\n",
    "                \n",
    "            results[\"audio_analyses\"] = audio_analyses\n",
    "            context.append(f\"Audio Analysis: {' '.join(audio_analyses)}\")\n",
    "        \n",
    "        # Process video if provided\n",
    "        if video:\n",
    "            if isinstance(video, str) and (video.startswith('http://') or video.startswith('https://')):\n",
    "                video_analysis = self.video_processor.analyze_video(youtube_url=video)\n",
    "            else:\n",
    "                video_analysis = self.video_processor.analyze_video(video_path=video)\n",
    "                \n",
    "            results[\"video_analysis\"] = video_analysis\n",
    "            if \"final_analysis\" in video_analysis:\n",
    "                context.append(f\"Video Analysis: {video_analysis['final_analysis']}\")\n",
    "        \n",
    "        # Generate a unified analysis if query is provided and we have processed multiple types\n",
    "        if query and len(context) > 1:\n",
    "            unified_prompt = f\"\"\"\n",
    "            Based on the following analyses of different content types, provide a unified response to this query: \"{query}\"\n",
    "            \n",
    "            {' '.join(context)}\n",
    "            \"\"\"\n",
    "            \n",
    "            results[\"unified_analysis\"] = self.text_model.generate_content(unified_prompt).text\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb87f3c",
   "metadata": {
    "papermill": {
     "duration": 0.008607,
     "end_time": "2025-04-10T09:45:42.225163",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.216556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ♻ *Practical Application Examples*\n",
    "\n",
    "*This section demonstrates the practical application of our multimodal analysis system through five comprehensive examples:*\n",
    "\n",
    "1. ***Document Processing**: Shows RAG implementation with climate change content*\n",
    "2. ***Image Analysis**:    Demonstrates visual content understanding*\n",
    "3. ***Audio Processing**:    Illustrates speech and sound analysis*\n",
    "4. ***Video Analysis**:    Shows combined visual and audio processing*\n",
    "5. ***Mixed Content Analysis**:    Demonstrates integrated multimodal analysis*\n",
    "\n",
    "*Each example is carefully chosen to showcase different aspects of the system's capabilities while providing practical insights into real-world applications.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763d57d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:42.243891Z",
     "iopub.status.busy": "2025-04-10T09:45:42.243567Z",
     "iopub.status.idle": "2025-04-10T09:45:42.252204Z",
     "shell.execute_reply": "2025-04-10T09:45:42.251311Z"
    },
    "papermill": {
     "duration": 0.020179,
     "end_time": "2025-04-10T09:45:42.253545",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.233366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_example():\n",
    "    \"\"\"Run an example to demonstrate the application capabilities\"\"\"\n",
    "    # Initialize the application\n",
    "    app = MultimodalContentHub(GOOGLE_API_KEY)\n",
    "    \n",
    "    # Example 1: Document processing and RAG\n",
    "    print(\"Example 1: Document processing and RAG\")\n",
    "    \n",
    "    # Sample document text\n",
    "    sample_document = \"\"\"\n",
    "    # Climate Change: A Global Challenge\n",
    "    \n",
    "    Climate change refers to long-term shifts in temperatures and weather patterns. \n",
    "    These shifts may be natural, but since the 1800s, human activities have been \n",
    "    the main driver of climate change, primarily due to the burning of fossil fuels \n",
    "    like coal, oil, and gas, which produces heat-trapping gases.\n",
    "    \n",
    "    ## Key Facts\n",
    "    \n",
    "    1. The Earth's average temperature has increased by about 1°C since pre-industrial times.\n",
    "    2. The past decade (2011-2020) was the warmest on record.\n",
    "    3. Sea levels have risen by about 20 cm since 1900.\n",
    "    4. The Arctic is warming twice as fast as the global average.\n",
    "    \n",
    "    ## Impacts\n",
    "    \n",
    "    Climate change affects every region of the world. The impacts include:\n",
    "    \n",
    "    - More frequent and intense droughts, storms, and heat waves\n",
    "    - Rising sea levels\n",
    "    - Melting ice caps and glaciers\n",
    "    - Loss of biodiversity\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the document\n",
    "    app.document_processor.process_text_string(sample_document)\n",
    "    \n",
    "    # Query the document\n",
    "    query = \"What are the impacts of climate change?\"\n",
    "    response = app.query_document(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response:\\n{response}\\n\")\n",
    "    \n",
    "    # Example 2: Image understanding\n",
    "    print(\"Example 2: Image understanding\")\n",
    "    \n",
    "    # Simulate image analysis with a text description\n",
    "    image_description = \"\"\"\n",
    "    This image shows a busy urban street scene with tall skyscrapers in the background.\n",
    "    There are several pedestrians walking on the sidewalk, and cars and buses on the road.\n",
    "    There's a traffic light showing red at an intersection, and some street vendors selling food.\n",
    "    The sky is clear blue, suggesting it's daytime.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Since we can't provide actual images, we'll simulate the analysis\n",
    "    prompt = f\"Analyze this image based on the description: {image_description}\"\n",
    "    response = app.text_model.generate_content(prompt).text\n",
    "    print(f\"Simulated image analysis result:\\n{response}\\n\")\n",
    "    \n",
    "    # Example 3: Audio understanding with function calling\n",
    "    print(\"Example 3: Audio understanding with function calling\")\n",
    "    \n",
    "    # Simulate audio processing\n",
    "    audio_path = \"simulated_audio.wav\"  # This file doesn't need to exist for the simulation\n",
    "    query = \"Transcribe this audio and tell me the main topics discussed\"\n",
    "    \n",
    "    # Simulate the audio transcription and analysis\n",
    "    response = app.analyze_audio(audio_path, query)\n",
    "    print(f\"Audio analysis result:\\n{response}\\n\")\n",
    "    \n",
    "    # Example 4: Video understanding\n",
    "    print(\"Example 4: Video understanding\")\n",
    "    \n",
    "    # Simulate video analysis with a YouTube URL\n",
    "    youtube_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"  # Example URL\n",
    "    video_analysis = app.video_processor.analyze_video(youtube_url=youtube_url)\n",
    "    \n",
    "    if \"error\" in video_analysis:\n",
    "        print(f\"Error analyzing video: {video_analysis['error']}\")\n",
    "    else:\n",
    "        print(f\"Video analysis result:\\n{video_analysis['final_analysis']}\\n\")\n",
    "    \n",
    "    # Example 5: Mixed content analysis\n",
    "    print(\"Example 5: Mixed content analysis\")\n",
    "    \n",
    "    # Simulate mixed content: text, image, and audio\n",
    "    mixed_query = \"Summarize the main points from the provided content.\"\n",
    "    text_content = \"Climate change is a pressing issue that requires immediate action.\"\n",
    "    image_description = \"An image showing a polar bear on a melting ice cap.\"\n",
    "    audio_path = \"simulated_audio.wav\"\n",
    "    \n",
    "    # Analyze mixed content\n",
    "    mixed_analysis = app.analyze_mixed_content(\n",
    "        text=text_content,\n",
    "        images=[image_description],  # Pass as list\n",
    "        audio=[audio_path],\n",
    "        query=mixed_query\n",
    "    )\n",
    "    \n",
    "    print(f\"Mixed content analysis result:\\n{mixed_analysis.get('unified_analysis', 'No unified analysis generated')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b424eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:45:42.270860Z",
     "iopub.status.busy": "2025-04-10T09:45:42.270552Z",
     "iopub.status.idle": "2025-04-10T09:46:11.123602Z",
     "shell.execute_reply": "2025-04-10T09:46:11.121967Z"
    },
    "papermill": {
     "duration": 28.864813,
     "end_time": "2025-04-10T09:46:11.126501",
     "exception": false,
     "start_time": "2025-04-10T09:45:42.261688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Document processing and RAG\n",
      "Query: What are the impacts of climate change?\n",
      "Response:\n",
      "The impacts of climate change include:\n",
      "\n",
      "- More frequent and intense droughts, storms, and heat waves\n",
      "- Rising sea levels\n",
      "- Melting ice caps and glaciers\n",
      "- Loss of biodiversity\n",
      "\n",
      "\n",
      "Example 2: Image understanding\n",
      "Simulated image analysis result:\n",
      "Okay, based on that description, here's a breakdown of what I'm \"seeing\" (or, rather, what the image likely contains) and some implications:\n",
      "\n",
      "**Key Elements and Their Implications:**\n",
      "\n",
      "*   **Busy Urban Street Scene:** This immediately sets the tone. It suggests a place with high population density, commerce, and activity. This implies a modern city, likely a major metropolitan area.\n",
      "\n",
      "*   **Tall Skyscrapers in the Background:** Skyscrapers are a hallmark of large, developed cities. Their presence signifies economic power, vertical space utilization, and architectural ambition. They emphasize the modern urban setting.\n",
      "\n",
      "*   **Several Pedestrians Walking on the Sidewalk:** The presence of pedestrians indicates a walkable city, where people move around not just by car but also on foot. \"Several\" suggests a decent level of foot traffic, reinforcing the \"busy\" aspect.\n",
      "\n",
      "*   **Cars and Buses on the Road:** This confirms the presence of vehicular traffic, both private (cars) and public (buses). This contributes to the sense of a bustling city. Buses suggest a public transportation system.\n",
      "\n",
      "*   **Traffic Light Showing Red at an Intersection:** This is a standard feature of urban environments, regulating traffic flow. The red light implies the potential for traffic congestion. The intersection is a focal point for activity.\n",
      "\n",
      "*   **Street Vendors Selling Food:** Street vendors add character to the scene. They suggest a vibrant street culture, affordable food options, and a degree of informality within the urban landscape. They also contribute to the overall sense of activity and commerce.\n",
      "\n",
      "*   **Clear Blue Sky, Suggesting Daytime:** This indicates good weather, providing a sense of energy and optimism to the scene. It also helps create a feeling of clarity and openness, which is contrasted by the density of the surrounding built environment.\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "The image, as described, depicts a thriving and dynamic urban environment. It's a classic picture of a modern city, with a blend of modern architecture, transportation, commerce, and human activity. The elements work together to create a lively and potentially overwhelming scene. There's a sense of both opportunity and the potential for congestion and stress inherent in urban life.\n",
      "\n",
      "**Possible Interpretations/Inferences:**\n",
      "\n",
      "*   **Economic Activity:** The presence of skyscrapers, street vendors, cars, and buses suggests a strong local economy with diverse industries.\n",
      "*   **Social Dynamics:** The mix of pedestrians, vendors, and drivers suggests a diverse population, possibly representing different socioeconomic groups.\n",
      "*   **Urban Planning:** The infrastructure (roads, sidewalks, traffic lights) speaks to the presence of urban planning and regulation.\n",
      "*   **Environmental Considerations:** While the blue sky suggests good weather, a realistic image might also show signs of pollution (air, noise) associated with urban areas.\n",
      "\n",
      "In short, the description paints a picture of a vibrant, complex, and bustling urban landscape.\n",
      "\n",
      "\n",
      "Example 3: Audio understanding with function calling\n",
      "Audio analysis result:\n",
      "Here's the transcription and the main topics discussed in the audio:\n",
      "\n",
      "**Transcription:**\n",
      "\n",
      "(background: keyboard clicks, muffled conversation) ...Alright, so let's circle back to the Q3 marketing campaign. We need to analyze the ROI and see where we can optimize. I think focusing on social media engagement is key... (brief pause) ...Yes, Sarah, you had a point?\n",
      "\n",
      "**Main Topics Discussed:**\n",
      "\n",
      "*   Q3 Marketing Campaign (Overall Performance)\n",
      "*   ROI Analysis (Return on Investment)\n",
      "*   Optimization Strategies\n",
      "*   Focus on Social Media Engagement\n",
      "\n",
      "\n",
      "Example 4: Video understanding\n",
      "Video analysis result:\n",
      "Here's a structured analysis of the simulated video, based on the provided frame analyses and audio transcription:\n",
      "\n",
      "**1. Overall Video Summary:**\n",
      "\n",
      "The video \"Simulated Video dQw4w9WgXcQ\" by \"Simulated Channel\" appears to be a product demonstration or informational presentation, likely focused on an electronic device. It follows a fairly standard format for this type of content: introduction of a presenter, demonstration of a product accompanied by supporting data (graphs and competitive analysis), and a concluding call-to-action.  The audio track, however, contains a completely different topic relating to hiking safety and preparation. This mismatch suggests a potential error in the provided data, a deliberately misleading video (akin to a Rickroll, given the \"dQw4w9WgXcQ\" in the title), or a test scenario.\n",
      "\n",
      "**2. Main Visual Elements and How They Change Over Time:**\n",
      "\n",
      "The video's visual elements evolve as follows:\n",
      "\n",
      "*   **0:30:** Introduction: A presenter is shown in front of a blue background, presumably introducing the video's topic or the product being featured. The presence of a microphone suggests a speaking presentation.\n",
      "*   **2:00:** Data Presentation: A graph demonstrating an upward trend is displayed. The colored lines and chart legend indicate different data points being compared, potentially showcasing product performance or market growth.\n",
      "*   **3:30:** Product Demonstration: The presenter interacts with a small electronic device, likely highlighting its features and functionality.  The touchscreen and hand gestures suggest an interactive user experience.\n",
      "*   **5:00:** Competitive Analysis: A table compares the featured product with competitors' offerings. Checkmarks likely signify superior features or benefits. Product icons and text provide specific details about the comparison.\n",
      "*   **6:30:** Call to Action: The video concludes with a slide displaying contact information, social media handles, and a logo, encouraging viewers to engage with the brand or purchase the product.\n",
      "\n",
      "In essence, the video transitions from an introductory scene to data-driven arguments, then to a practical demonstration, followed by a competitive overview, and finally to a direct appeal for engagement. This is a typical progression for product marketing or informational videos.\n",
      "\n",
      "**3. Main Topics Discussed in the Audio:**\n",
      "\n",
      "The audio track is completely focused on *Hiking Safety/Preparation*, specifically:\n",
      "\n",
      "*   **Hydration:** The importance of consistently drinking water, even before feeling thirsty.\n",
      "*   **Electrolyte Balance:** The need to replenish electrolytes during long hikes to prevent feeling unwell.\n",
      "*   **Pre-Planning:** Emphasizing that hydration and electrolyte management are crucial aspects of planning a safe and successful hike.\n",
      "\n",
      "**4. Overall Mood/Tone of the Video:**\n",
      "\n",
      "Based purely on the **visual description**, the mood and tone appear to be:\n",
      "\n",
      "*   **Professional and Informative:**  The presentation setting, graphs, and competitive analysis suggest a factual and data-driven approach.\n",
      "*   **Promotional and Persuasive:** The product demonstration and call-to-action indicate a clear intention to showcase and sell a product.\n",
      "\n",
      "However, the discrepancy between the visual and auditory elements significantly alters this interpretation. Given the video title \"dQw4w9WgXcQ\", it is HIGHLY probable the real mood is intentionally *humorous or deceptive*. The incongruity between what you *expect* to see (a product demo) and the actual audio (hiking tips) points to a deliberate attempt to subvert expectations, characteristic of a \"Rickroll\" or similar bait-and-switch tactic.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The analysis reveals a likely case of a deliberate mismatch. The visual content seems to be a standard product presentation, while the audio is entirely unrelated (hiking safety). While the visual content *appears* professional and promotional, the *overall* mood and tone, considering the title and audio mismatch, strongly suggests a prank, a test scenario, or an error in the data provided. The true intention of the video cannot be definitively determined without actually viewing it. If dQw4w9WgXcQ is the actual YouTube ID, then it is indeed a Rickroll.\n",
      "\n",
      "\n",
      "Example 5: Mixed content analysis\n",
      "Mixed content analysis result:\n",
      "The provided content addresses three different types of information:\n",
      "\n",
      "*   **Text:** Climate change is a long-term shift in temperature and weather patterns caused primarily by human activities (burning fossil fuels). Earth's average temperature has increased by 1°C since pre-industrial times, and the past decade has been the warmest on record. Sea levels have risen by 20 cm since 1900, and the Arctic is warming at twice the global average rate. Impacts include extreme weather, rising sea levels, melting ice, and biodiversity loss.\n",
      "\n",
      "*   **Image:**  No information is available from the image, as I am unable to process image data.\n",
      "\n",
      "*   **Audio:** The audio is a simulated business meeting discussing a Q3 report, including speaker identification, background noise, and overlapping speech. The specific content of the Q3 report discussion is not detailed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_example()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 147.139512,
   "end_time": "2025-04-10T09:46:14.054373",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-10T09:43:46.914861",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
